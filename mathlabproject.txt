Data mining II Term project

Instructor Dr. NAIT-HAMOUD Department of Second Cycle - fourth year Applied statistics National higher School of Mathematics Algiers, Algeria Dec 22, 2025 Academic year 2025/2026 1
1 INTRODUCTION

The aim of this project is to implement an entity linking system. Entity Linking (EL) is an information extraction task that links entity mentions present in a text to knowledge base entries such as Wikipedia, Dbpedia, etc. Figure 2 illustrates this task, the chunks of text highlighted in green represent mentions to entities coherent with the context of the respective texts where they occur. Mainly, the EL task can be broken down into two main sub-tasks.

– First, entity mentions (highlighted in the figure in green) must be located in the text, we refer to this sub-task as ”recognition,” – Second, those mentions must be associated to the appropriate Wikipedia page.

Figure 1: Example of linking mention to entities (Wikipedia page in this case)

Wikpedia knowledge base include three type of pages.

    Redirect pages: redirects are used to make searching for information in Wikipedia easier,
    Disambiguation pages (pages that do not cover a unique topic, but help to find the specific article when multiple topics share the same name),
    Proper Wikipedia pages that cover a unique topic (see definition 2 of section 4 of the provided paper).

One of the methods used by Wikipedia to search for pages is called direct matching and redirection that consists in two alternatives. – If your search query exactly matches an article title, Wikipedia usually navigates you directly to that page instead of showing results (Exact Matches).

Academic year 2025/2026 2
1 INTRODUCTION (Continued)

– It checks for ”redirects”—alternative titles (like ”Alan Mathison Turing” redirecting to ”Alan Turing”) to ensure you find the correct page even with variations (redirects).

For instance, figure 2 depicts the ”Alan Turing” Wikipedia page. The information surrounded in red shows that there is a redirect page with the title ”Turing” that redirects to the proper page of ”Alan Turing”. Besides, the information surrounded in green informs that there is a disambiguation page for the word ”Turing”. Figure 3 shows a snapshot of the disambiguation page corresponding to ”Turing”.

Figure 2: Example of a Wikipedia page: the text surrounded in blue indicates an example of an anchor (clickable text used to refer to a given Wikipedia page) Figure 3: a snapshot of ”Turing” Disambiguation page

Academic year 2025/2026 3
2 ENTITY EXTRACTION AND DISAMBIGUATION

The EL system TELS (for Tweet Entity Linking System) introduced in the paper of reference [1] is and end-to-end system. This means that the two phases namely: mention extraction (or recognition) and linking are carried out in one shot. The authors in [1] did not extract the relevant mentions as a first phase, and in a second phase link the relevant extracted mention with the appropriate Wikipedia page.

After a preprocessing phase, described in [1], the authors extract the Ngrams from the cleaned text using a function similar to the one depicted in figure 4. Then, each Ngram is looked for in the inverted index, if it exists; candidate pairs (Ngram,PageID) are generated for each Page referenced by the Ngram. Afterwards, for each pair (Ngram,PageID) a feature vector is build on the basis of the selected features (see the work in [1]). Finally, a Model is trained using the provided revised Meij dataset.

Figure 4: A python function to extract Ngrams from a text. If the Ngram position in the text is required the code should be enhanced

The provided material (files) for this project include the paper on TELS system [1] (to read carefully), prebuilt datasets (PostingsLast and PageIdToContext2), the python files to be used for reading records from these datasets, the gold standards, and the Meij dataset (multiple versions).
2.1 Provided prebuilt datasets and manipulation python code

For the needs of the project, the following datasets have been made available for you.

– Inverted index: A dictionary of anchors including redirect page titles was created from the English Wikipedia dump of January 2019. The dictionary of anchors is a key-value store that associates to each entry (Wikipedia anchor or redirect page title) the set of Wikipedia pages it refers to, along with a score representing the number of times the entry was used as cross-reference to each page.

The provided files to use for manipulating the inverted index are:

    The Lmdb PostingsLast: To implement the database, LMDB (Lightening-Mapped Database) storage engine was used due to its high read performances. Moreover, to improve serialization, Google Protocol Buffers were used due to their best performances compared with XML, JSON and other existing formats.
    InvertedIndexAccess.py: can be used, through the function call IndexAccess(ngram,trxn), to read from the lmdb PostingsLast. When you run the python code in InvertedIndexAccess.py,

Academic year 2025/2026 4
2.1 Provided prebuilt datasets and manipulation python code (Continued)

you are asked to enter a mention (text) m or an ngram, if this mention was used as an anchor, or a redirect page, in Wikipedia to refer to specific pages, the result is a list of records. Each record include the page ID of the referenced page, the score and the type of the referenced page.

– pageId: The ID of the Wikipedia page – score: It tells you how many time the text or the mention m was used, as an anchor, in the whole Wikipedia to refer to this page. – type: 0 for anchor (the Ngram was used as an anchor), 1 for redirect (the Ngram was used as a redirect page title) and 2 for both (the Ngram was used as a redirect page title and as an anchor to cross-reference the page)

    SerializedListNew.pb2 (it is the defined Google protocol buffer), it should be imported in the file InvertedIndexAccess)

Figure 5: Example of a search for the Ngram ”cryptanalyst” in the inverted index

– The dataset of contexts PageIdToContext2: It is also a key-value store that includes the following information for each Wikipedia page:

    Page title,
    List of anchors and redirect page titles that link to this page,
    Categories of the page included in the Wikipedia categories section,
    list of anchors and titles of Wikipedia pages referenced in the abstract,
    number of times the page was visited during the year 2019,
    Page rank of the page in the Wikipedia knowledge base graph.

– InterrogatePageIdToContextLmdb.py: This python code allows for interrogating the database (lmdb) PageIdToContext2 that represents the dataset of contexts of each Wikipedia page. Figure 6 shows an example of the execution of this python code.

Each Wikipedia page include a ”categories” section. This latter is a navigational system used to group related articles. They allow to browse through related topics even if they do not know the exact title of a specific page. Figure 7 shows the ”categories” section of the Wikipedia page entitled ”Alan Turing”.

Academic year 2025/2026 5
2.2 Gold standards and training set

Figure 6: Example of the execution of the python code InterrogatePageIdToContextLmdb.py Figure 7: Categories of the Wikipedia page corresponding to Alan Turing

The gold standards and the Meij datasets, used in [1], are also provided for the comparison of your results with the baselines: TagMe, AIDA, DBpedia Spotlight and WAT.

Each gold standard is provided in a separate folder that includes two files: – a ”tsv” file for the tweet texts – a ”tsv” file for the annotations. ˆ Remark: The columns in the annotation file of the gold standards are not the same.

Academic year 2025/2026 6
3 Required tasks

    Train a model, using Meij revised dataset, with at least two different techniques seen and experienced during lab and lecture sessions (DNN is mandatory). your code must be modular and include: – a text processing module, – a feature extraction module (use at least 7 features defined in [1] )
    Test your models on all the provided gold standards, as in the work in [1] )
    Compare your implemented EL system with TagMe and AIDA.
    Provide a detailed and well-organized report on your project, including an argumentation for all your choices.

